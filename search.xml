<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CLIP-笔记</title>
    <url>/2022/03/29/CLIP-%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="Learning-Transferable-Visual-Models-From-Natural-Language-Supervision"><a href="#Learning-Transferable-Visual-Models-From-Natural-Language-Supervision" class="headerlink" title="Learning Transferable Visual Models From Natural Language Supervision"></a>Learning Transferable Visual Models From Natural Language Supervision</h2><p><a href="G:\Google\2103.00020.pdf">paper</a> <a href="https://github.com/OpenAI/CLIP">Github</a> </p>
<p><img src="/CLIP.png" alt="avatar" title="模型总览与zero-shot推理"></p>
<hr>
<span id="more"></span>

<h3 id="关键点"><a href="#关键点" class="headerlink" title="关键点"></a>关键点</h3><ol>
<li>结合自然文本和图片生成数据集</li>
<li>无需人工标注</li>
<li>对比学习</li>
<li>多模态工作</li>
<li>标签不是提前定义好的列表式的标签</li>
</ol>
<h3 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h3><ol>
<li>之前通过自然语言协助图片分类的工作有着最大的问题就是规模不够大</li>
<li>之前的工作多是用文本带来的弱监督信号去帮助图片的有监督学习，但是这类任务仍然针对的是固定类别的类，没有zero-shot的能力</li>
</ol>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><font color="red" size="4">概述</font></p>
<ol>
<li>核心是用自然语言的有监督讯号训练视觉任务</li>
<li>Transformer和BERT的兴起使得自然语言模型更加强大</li>
<li>数据集有4个亿的图片单词数据对</li>
<li>训练效率对多模态工作的结果非常重要</li>
<li>预测学习（给定图片预测文本）与对比学习相比需要巨大的计算资源</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># image_encoder - ResNet or Vision Transformer</span><br><span class="line"># text_encoder - CBOW or Text Transformer</span><br><span class="line"># I[n, h, w, c] - minibatch of aligned images</span><br><span class="line"># T[n, l] - minibatch of aligned texts</span><br><span class="line"># W_i[d_i, d_e] - 将图片特征投影到多模态的空间</span><br><span class="line"># W_t[d_t, d_e] - 将文本特征投影到多模态的空间</span><br><span class="line"># t - 学得的温度参数 ???</span><br><span class="line"></span><br><span class="line"># 抽取特征</span><br><span class="line">I_f = image_encoder(I) # [n, d_i]</span><br><span class="line">T_f = text_encoder(T) # [n, d_t]</span><br><span class="line"></span><br><span class="line"># 使模型能够学习到多模态的特征</span><br><span class="line">I_e = l2_normalize(np.dot(I_f, W_i), axis=1)</span><br><span class="line">T_e = l2_normalize(np.dot(T_f, W_t), axis=1)</span><br><span class="line"></span><br><span class="line"># 算下相似度</span><br><span class="line">logits = np.dot(I_e, T_e) * np.exp(t)</span><br><span class="line"></span><br><span class="line"># 生成真值并计算损失</span><br><span class="line">labels = np.arange(n) # 因为都是在对角线上配的的</span><br><span class="line">loss_i = cross_entropy_loss(logits, labels, axis=0)</span><br><span class="line">loss_t = cross_entropy_loss(logits, labels, axis=1)</span><br><span class="line">loss = (loss_i + loss_t) / 2</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>在这个工作中，最后用的是线性投射层，而没有使用非线性透射层</li>
</ol>
<p><font color="red" size="4">训练</font></p>
<ol>
<li>调参都是用小模型训练一个周期的结果来调的</li>
<li>batch_size &#x3D; 32768</li>
<li>混精度训练 (???)</li>
<li>GPU并行计算</li>
</ol>
<p><a href="https://lilianweng.github.io/posts/2021-09-25-train-large/">大模型训练技巧</a></p>
<ol start="5">
<li>训练ViT比训练残差网络效率更高</li>
<li>最后又在更大的图片上微调了一下</li>
</ol>
<p>P.S. 可以用FFiNet@1hop区分一个模型</p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><ol>
<li>如何作zero-shot推理?</li>
</ol>
<p>A: 图片通过编码器得到一个特征，感兴趣的类别输入到”A photo of a {object}”中，然后经过一个文本编码器，得到n个特征，将这n个特征和图片特征点乘后作Softmax，得到在每个类别的概率。<br>P.S. 为什么不直接用这个类别这一个单词呢？因为训练就是用的句子，存在distribution gap。</p>
<ol start="2">
<li>Prompt (提示) engineering and ensembling ???</li>
</ol>
<ul>
<li>语言歧义性的问题，比如boxer可以是一种狗也可以是拳击运动员。使用”A photo of a {object}”则使得object一定是名词，因此一定程度上缓解了语言歧义性的问题。</li>
<li>另外在宠物的数据集加上一句”a type of pet”作为提示也会得到更好的效果</li>
<li>对prompt进行ensemble，即使用很多prompt，最后的结果取他们的平均，则会得到更好的效果。本文用了80个提示模板<br>P.S. Linear probe指冻住主干网络，只微调最后一个全连接层的操作</li>
</ul>
<ol start="3">
<li>用全部数据和之前的特征学习方法进行对比</li>
</ol>
<ul>
<li>仍然使用linear probe而不采用fine-tune，因为fine-tune要训练整个网络，这样就不能分辨预训练模型的好坏了；另外也不用调参。</li>
<li>模型非常好</li>
</ul>
<h3 id="Limitation"><a href="#Limitation" class="headerlink" title="Limitation"></a>Limitation</h3><ol>
<li>和各个数据集的SOTA结果还是有一定差距的，扩大规模不太现实</li>
<li>在有些数据集上表现不太好，无法处理抽象的概念（比如数物体或者判断异常）</li>
<li>在MNIST这类数据集上不太好，和自然图像还是有点差距的，是一个out-of-distribution的数据集</li>
<li>只是从给定类别去做推理，而不能直接写出图片的标签</li>
<li>对数据的利用不是很高效，可以用数据增强、伪标签或自监督减小数据用量</li>
<li>数据没有经过清洗，很可能会带一些社会偏见</li>
<li>很多任务用语言都无法描述的</li>
<li>如何在few-shot也能具有更好的效果</li>
</ol>
<center><font color="brown" size="5"> 打破了固定标签类别的范式 </font></center>

]]></content>
      <categories>
        <category>视觉算法</category>
      </categories>
  </entry>
  <entry>
    <title>MoCo-笔记</title>
    <url>/2022/03/29/MoCo-%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning"><a href="#Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning" class="headerlink" title="Momentum Contrast for Unsupervised Visual Representation Learning"></a>Momentum Contrast for Unsupervised Visual Representation Learning</h2><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html">paper</a> <a href="https://github.com/facebookresearch/moco">code</a></p>
<p><img src="/images/MOCO.png" alt="avatar" title="模型总览与zero-shot推理"></p>
<hr>
<span id="more"></span>

<h3 id="什么是对比学习"><a href="#什么是对比学习" class="headerlink" title="什么是对比学习"></a>什么是对比学习</h3><ol>
<li>通过相似度来进行无监督学习</li>
<li>通过代理任务来得到两个图片是否相似，如instance discrimination：将一张图片进行随机裁剪和数据增强得到两个处理后的图片，将这两张图片认为是正样本，其他图片对这两张图片而言是负样本</li>
<li>对比学习在找完正负样本后所做的就是抽取特征，接着使用常见的损失函数如NCE反向传播</li>
<li>关键在于找到定义正样本和负样本的规则(代理任务, pretext tasks)</li>
</ol>
<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><ol>
<li>无监督模型在NLP上的表现很好，如GPT，BERT</li>
<li>对比学习可以被当做动态字典查询问题，即对于目标图片特征(anchor&#x2F;key)寻找与最相似的图片特征(positive-negative&#x2F;value)</li>
<li>动态字典越大越好-能够更好地在高维空间进行采样</li>
<li>字典中的keys应该用相同或相似的编码器得到</li>
<li>受限于显存，用队列表示keys，即得到mini-batch样本的特征后，将其放入队列，然后把最早的mini-batch的特征移出队列</li>
<li>使用队列的话字典里的特征就不一致了，因为一部分来自老的，一部分来自新的，采用动量的编码器解决这个问题，即参数一部分来query的编码器，一部分来自前一步的参数，通过调整两者的比例，也就是使参数更多地来自前一步的参数，那么参数将会更新地非常缓慢，从而缓解了队列特征不一致的问题</li>
<li>代理任务选择的是instance discrimination方法</li>
<li>最后MoCo可以在中型数据集ImageNet或者大型数据集Instagram image得到非常好的效果</li>
</ol>
<h3 id="讨论和结论"><a href="#讨论和结论" class="headerlink" title="讨论和结论"></a>讨论和结论</h3><ol>
<li>数据集的增大对于效果的提升没有很多，可能是代理任务的问题</li>
<li>有没有可能把MoCo和NLP里的masked auto-encoding结合起来 (kaiming大神-MAE)</li>
</ol>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><ol>
<li>noise contrasive estimation (NCE)，能够解决softmax类别过多而无法过大的问题。NCE将多分类问题当作一个二分类问题，一共有两类，一类是data sample，另一类是noisy contrastive。</li>
<li>InfoNCE<br>$$\mathcal{L}<em>{q}&#x3D;-\log \frac{\exp \left(q \cdot k</em>{+} &#x2F; \tau\right)}{\sum_{i&#x3D;0}^{K} \exp \left(q \cdot k_{i} &#x2F; \tau\right)}$$<br>where $\tau$ 是一个温度超参数，也就是控制分布的形状，$K$指的是负样本的数量</li>
<li>动量对比<ul>
<li>使用队列能够使字典大小和batch size分离开，从而使用标准的batch size。</li>
<li>使用队列能够使字典使用之前编码好的key，而不用重新进行计算</li>
<li>字典是整个数据集的一个子集，对应了前面提到的NCE中的estimation，即只选用一部分样本作为负例，从而减小计算开销</li>
<li>使用队列可以移走最老的特征，从而保持了字典的一致性，即几乎都是用一个编码器编的</li>
<li>因为这个队列非常大，因此很难去通过反向传播去更新所有key的编码器</li>
<li>能不能直接吧query的编码器直接给key用呢？结果并不好，可能是因为编码器太快改变了，因此使得队列中元素的一致性遭到破坏。由此引出动量更新：$\theta_{\mathrm{k}} \leftarrow m \theta_{\mathrm{k}}+(1-m) \theta_{\mathrm{q}}$</li>
<li>用一个非常大的动量(0.999)就可以使得参数更新得很慢</li>
<li>端到端的对比学习，也就是key和query的编码器使用同一个，并使用梯度回传更新参数。优点是字典的一致性非常高，缺点就是字典的大小需要和mini-batch的size一样，从而限制了字典的大小</li>
<li>memory bank。对于query使用编码器，对于key建立一个memory bank，这个bank是将所有的key储存起来，接着在训练的时候抽样一些算出来contrastive loss后更新query的编码器，接着用这个编码器将抽样后的样本重新计算特征，将计算完的特征再扔回memory bank里。这样会使得key的特征的一致性非常差。同时不太容易处理数据集非常大的任务</li>
</ul>
</li>
</ol>
<p>P.S. trivial solution - 捷径解 (TSTiNet)</p>
<ol start="4">
<li>伪代码</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># f_q, f_k: encoder networks for query and key</span><br><span class="line"># queue: dictionary as a queue of K keys (CxK)</span><br><span class="line"># m: momentum</span><br><span class="line"># t: temperature</span><br><span class="line"></span><br><span class="line">f_k.param = f_q.params # initialize</span><br><span class="line">for x in loader: # load a minibatch x with N samples</span><br><span class="line">	x_q = aug(x) # a randomly augmented version</span><br><span class="line">	x_k = aug(x) # another randomly augmented version</span><br><span class="line"></span><br><span class="line">	q = f_q.forward(x_q)  # queries: NxC</span><br><span class="line">	k = f_k.forward(x_k)  # keys: NxC</span><br><span class="line">	k = k.detach() # k进行梯度回传操作</span><br><span class="line"></span><br><span class="line">	# positive logits: Nx1</span><br><span class="line">	l_pos = bmm(q.view(N, 1, C), k.view(N, C, 1))</span><br><span class="line"></span><br><span class="line">	# negative logits: NxK</span><br><span class="line">	l_neg = mm(q.view(N, C), queue.view(C, K))</span><br><span class="line"></span><br><span class="line">	# logits: Nx(1+K)</span><br><span class="line">	logits = cat([l_pos, l_neg], dim=1)</span><br><span class="line"></span><br><span class="line">	# contrastive loss</span><br><span class="line">	labels = zeros(N) # positives are the 0-th</span><br><span class="line">	loss = CrossEntropyLoss(logits/t, labels)</span><br><span class="line"></span><br><span class="line">	# query网络的更新</span><br><span class="line">	loss.backward()</span><br><span class="line">	update(f_q.params)</span><br><span class="line"></span><br><span class="line">	# momentum update: key network</span><br><span class="line">	f_k.params = m*f_k.params + (1-m)*f_q.params</span><br><span class="line"></span><br><span class="line">	# 更新字典</span><br><span class="line">	enqueue(queue, k) # 让minibatch进入队列</span><br><span class="line">	dequeue(queue) # 让minibatch走出队列</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>Shuffle BN: 为了防止模型学到捷径，因为BN会算当前样本的running mean和running variance，从而泄露信息，那么模型就会根据这些泄露的信息很容易找到那个正样本。因此Shuffle BN采用先把样本的顺序打乱，送到各个GPU上，最后再恢复顺序</li>
</ol>
<h3 id="实验与结果"><a href="#实验与结果" class="headerlink" title="实验与结果"></a>实验与结果</h3><ol>
<li>在ImageNet-1k和Instegram-1B进行训练</li>
<li>学习率设为了30，可能有监督对比学习和无监督对比学习学到的特征非常不一致</li>
<li>MoCo的扩展性好，硬件要求低</li>
<li>无监督学习最主要的目标最重要的就是生成可泛化的特征</li>
<li>用了特征归一化，然后用有监督训练的超参数做微调</li>
</ol>
]]></content>
      <categories>
        <category>视觉算法</category>
      </categories>
  </entry>
  <entry>
    <title>搭建个人博客</title>
    <url>/2022/03/29/%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<p>这是个人博客搭建的向导</p>
<span id="more"></span>

<h3 id="相关工具下载"><a href="#相关工具下载" class="headerlink" title="相关工具下载"></a>相关工具下载</h3><ul>
<li><a href="https://nodejs.org/en/">Node.js</a> ：使用<code>node -v</code>和<code>npm -v</code>查看是否安装成功</li>
<li><a href="https://git-scm.com/download/win">git</a>：使用<code>git --version</code>查看是否安装成功</li>
<li>Hexo：<code>npm install -g hexo-cli</code></li>
</ul>
<h3 id="Github仓库搭建"><a href="#Github仓库搭建" class="headerlink" title="Github仓库搭建"></a>Github仓库搭建</h3><ul>
<li>新建一个仓库，仓库名为<code>&lt;Github用户名&gt;.github.io</code></li>
<li>连接本地与Github服务器：<br>1. 打开Gitbash输入<code>ssh-keygen -t rsa -C &quot;&lt;Github注册邮箱地址&gt;&quot;</code><br> 2. 在<code>C:/users/&lt;用户名&gt;/.ssh</code>文件夹下，复制<em>id_rsa.pub</em>文件中的内容<br> 3. 拷贝内容至Github的settings中</li>
</ul>
<p>注：在.ssh新建文件config，在里面填入</p>
<figure class="highlight plaintext"><figcaption><span>github.com</span></figcaption><table><tr><td class="code"><pre><span class="line">User 注册github的邮箱</span><br><span class="line">Hostname ssh.github.com</span><br><span class="line">PreferredAuthentications publickey</span><br><span class="line">IdentityFile ~/.ssh/id_rsa</span><br><span class="line">Port 443</span><br></pre></td></tr></table></figure>

<h3 id="搭建个人博客"><a href="#搭建个人博客" class="headerlink" title="搭建个人博客"></a>搭建个人博客</h3><ul>
<li><code>hexo init</code> 初始化本地博客文件夹，注意该文件夹应为空</li>
<li><code>hexo g</code> 生成网页</li>
<li><code>hexo s</code> 把生成的网页放在本地服务器，输入[<a href="http://localhost:4000/]%E6%9F%A5%E7%9C%8B%E6%95%88%E6%9E%9C">http://localhost:4000/]查看效果</a></li>
<li><code>ctrl + c</code> 结束本地网页</li>
</ul>
<h3 id="更改主题-NexT"><a href="#更改主题-NexT" class="headerlink" title="更改主题 (NexT)"></a>更改主题 (NexT)</h3><ul>
<li>在本地博客文件夹下载next：<br><code>git clone https://github.com/theme-next/hexo-theme-next themes/next</code> </li>
<li>更改Hexo的config中的scheme值为next</li>
<li>修改头像：在next&#x2F;_config.yml中找到<code>avatar</code></li>
<li>添加搜索：在next&#x2F;_config.yml中找到<code>local search</code>，并根据上面网页下载搜索插件</li>
<li>添加分类：在next&#x2F;_config.yml中找到<code>menu</code>，取消掉相关注释，然后<code>hexo new page &lt;&gt;</code></li>
</ul>
<h3 id="其他操作"><a href="#其他操作" class="headerlink" title="其他操作"></a>其他操作</h3><ul>
<li><p>删除文件：删除本地文件后键入<code>hexo clean</code></p>
</li>
<li><p>将当前博客上传到github上</p>
<ol>
<li>安装部署工具：<code>npm install hexo-deployer-git --save</code></li>
<li>修改hexo的_config.yml文件中的deploy部分</li>
<li>使用<code>hexo clean</code>、<code>hexo g</code>、<code>hexo d</code></li>
</ol>
</li>
</ul>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习与量子物理</title>
    <url>/2022/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E9%87%8F%E5%AD%90%E7%89%A9%E7%90%86/</url>
    <content><![CDATA[<p><a href="https://link.springer.com/book/10.1007/978-3-030-40245-7">source</a></p>
<p>介绍机器学习在量子物理(主要是量子化学中的应用)，如分子性质预测等</p>
<span id="more"></span>

<h2 id="第2章-Introduction"><a href="#第2章-Introduction" class="headerlink" title="第2章 Introduction"></a>第2章 Introduction</h2><h3 id="2-2-结构-性质关系"><a href="#2-2-结构-性质关系" class="headerlink" title="2.2 结构-性质关系"></a>2.2 结构-性质关系</h3><ul>
<li>分子的反向设计通常难于正向预测</li>
<li><em>水蒸气</em>的性质可以只取决于单个水分子 (即两个O-H距离和一个H-O-H角度)，这些空间信息以极窄的分布存在；至于<em>液态水</em>，事实上单个水分子的状态仍没有什么变化，引起液态水和水蒸气性质差异的是分子间相互作用；至于<em>冰</em>，水分子内的自由度仍然没有改变，水分子间的自由度发生了变化，他们不像在气相或液相那样可以随意移动。冰是一种晶体，这意味着每个水分子以一定的周期形式排列着，单个晶体有时会对其性质产生影响。</li>
<li>至于其他的无定型固体，如蛋白质，它是由氨基酸序列折叠而来的，我们可以通过把蛋白质放到溶液中去结晶化，从而获得更加精确的分子结构。</li>
<li>分子中的自由度可以分为强自由度和弱自由度，弱自由度具有较大的方差(如分子间相互作用)，这种自由度可以不作为分子结构的构成；强自由度有着窄的概率分布(低方差)，且他们的平均值组成了分子。</li>
<li>通常我们考虑的结构-性质关系不考虑弱自由度。</li>
<li>一般针对正向问题和反向问题，我们的目标都是去求概率分布，而非一个确定的函数。</li>
<li>分子性质可以分为两类，电子性质和热力学性质，电子性质通常是由电子云分布决定的，热力学性质是由弱自由度(也就是原子核的移动)统计得到的。热力学性质可以分为平衡性质与不平衡性质，平衡性质比如说沸点、溶点等，非平衡性质指化学反应或者物质传递等。</li>
</ul>
<h3 id="2-3-量子力学"><a href="#2-3-量子力学" class="headerlink" title="2.3 量子力学"></a>2.3 量子力学</h3><ul>
<li>希尔伯特空间(Hilbert Space)<ol>
<li>是一个线性向量空间</li>
<li>向量间的内积满足如下性质<br>a) 共轭对称，即一对向量和他们交换后的复共轭向量是相等的 $\langle y, x\rangle&#x3D;\overline{\langle x, y\rangle}$<br>b) 线性的 $\left\langle a x_{1}+b x_{2}, y\right\rangle&#x3D;a\left\langle x_{1}, y\right\rangle+b\left\langle x_{2}, y\right\rangle$<br>c) 正定的 $\left\langle x,x\right\rangle&#x3D;|x|^2\ge 0$<br>d) 共轭线性的 $\left\langle x, a y_{1}+b y_{2}\right\rangle&#x3D;\bar{a}\left\langle x, y_{1}\right\rangle+\bar{b}\left\langle x, y_{2}\right\rangle$<br>e) 两点之间的距离定义为 $d(x,y)&#x3D;|x-y|&#x3D;\sqrt{\langle x-y, x-y\rangle}$</li>
<li>该空间是可分离的，包含一个可数的、稠密的子集</li>
<li>是完备的（没有间隔）</li>
</ol>
</li>
<li>在希尔伯特空间表示一个物体的状态通过下式：<br>$$|\psi\rangle:&#x3D;\int \operatorname{d\mathbf{r}} \psi(\mathbf{r})|\mathbf{r}\rangle$$</li>
<li>与特征值和特征向量满足的关系类似($\hat{L}\left|\psi_{i}\right\rangle&#x3D;\lambda_{i}\left|\psi_{i}\right\rangle$)，将其中的线性算子改为哈密顿算子(Hamiltonian)，对应的特征值变为能量E，并将状态用波函数$\psi(\mathbf{r})$来代替：<br>$$\hat{H} \psi_{i}(\mathbf{r})&#x3D;E_{i} \psi_{i}(\mathbf{r})$$</li>
<li>目前大部分的量子力学方法，如密度泛函等，最快也只能到O($N^3$)</li>
<li>哈密顿算子中取决于原子核位置的项被叫做势能面(potential energy surface)</li>
</ul>
<h3 id="统计力学"><a href="#统计力学" class="headerlink" title="统计力学"></a>统计力学</h3><ul>
<li>统计力学对于弱自由度上的移动的建模非常有效，核心思想就是大部分的移动都可以合理省去，与物理性质相关的特征可以用较少的自由度来表示。</li>
<li>统计力学最主要的结果就是：对于一个材料，给定一个温度，那么他的微观态的概率密度分布与玻尔兹曼因子成比例<br>$$P(\mathbf{s}) \propto \mathrm{e}^{-\frac{V(\mathbf{s})}{T}}$$</li>
<li>归一化的概率分布为：<br>$$Z(T)&#x3D;\int \mathrm{ds} \mathrm{}^{\frac{-E(\mathrm{<del>s})}{T}}, \quad P(\mathrm{</del>s})&#x3D;\frac{1}{Z(T)} \mathrm{e}^{-\frac{E(\mathrm{~s})}{T}}$$</li>
<li>然后平均能量就可以写为：<br>$$\langle E\rangle&#x3D;\int \mathrm{ds} P(\mathrm{<del>s}) E(\mathrm{</del>s})&#x3D;T^{2} \frac{\partial \ln Z}{\partial T}$$</li>
<li>统计力学的计算复杂度主要就集中于上面的积分过程，对于具有较多原子的材料也有着计算复杂度高的问题。但是这通常可以通过蒙特卡洛模拟来减少计算量，也就是选取一些代表性的sample，计算他们的概率。因此统计力学的任务就变成了得到一些统计意义上重要的微观状态(也就是有着大的玻尔兹曼因子的微观状态)。</li>
<li>最常用的得到这种微观状态的方法就是分子动力学，这种方法让原子受到势能面的影响而进行经典力学上的移动，然后我们可以通过截取原子们运动的快照来获得微观状态。</li>
</ul>
<h2 id="第3章-Kernel-Methods-for-Quantum-Chemistry"><a href="#第3章-Kernel-Methods-for-Quantum-Chemistry" class="headerlink" title="第3章 Kernel Methods for Quantum Chemistry"></a>第3章 Kernel Methods for Quantum Chemistry</h2><p>To be continued …</p>
]]></content>
      <categories>
        <category>量子力学</category>
      </categories>
  </entry>
</search>
